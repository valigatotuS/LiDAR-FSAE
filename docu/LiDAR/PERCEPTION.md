Perception refers to the process of interpreting and making sense of sensory data, in this case from the race circuit. It involves taking raw data from sensors, such as LiDAR cameras, and transforming it into meaningful information about the race track and its features, such as cones and other obstacles.

Perception can be achieved through a combination of **computer vision, machine learning, and robotics algorithms**. For example, computer vision algorithms can be used to detect and track cones in a point cloud generated by a LiDAR sensor. Machine learning algorithms can be used to classify cones and predict their future position based on training data. Robotics algorithms can be used to estimate the pose and position of the racecar on the circuit.

The goal of perception in Formula Student racecars is to provide the car with a **high-level understanding of the race track and its features**, including the location of cones, their shape and size for defining the cone type, and their behavior over time. This information is then used by the car to make decisions and control its actions on the circuit, such as adjusting its speed or trajectory to avoid cones or navigate tight turns.

<h2>Object Detection</h2>
LiDAR cameras provide 3D data of the environment as a series of points, known as "point clouds". The goal of perception is to interpret these point clouds to detect and classify objects in the scene.

One of the key steps in processing LiDAR data is to separate the background from the foreground, which can be achieved through techniques such as adaptive background removal. This step is crucial in determining which points belong to an object and which points do not. To accomplish this task, algorithms use methods such as clustering or segmentation, where similar points in the point cloud are grouped together based on their geometric or spatial properties. This allows the algorithm to identify objects in the scene and mark them with a bounding box, which helps in the localization and characterization of objects in the point cloud.

Once the objects are segmented, they are marked with a bounding box. This box helps in localizing the object and characterizing its shape. In the case of conical objects, such as cones, further processing may be required to accurately reconstruct and filter the cones. Additionally, color estimation can also be performed to enhance the perception of the objects.

There are several algorithms available for object detection using LiDAR point clouds. Some of the popular algorithms include PointNet, PointNet++, SECOND, PointRCNN, and F-PointNet, among others. These algorithms have varying levels of robustness, reliability, and extensibility, and the choice of algorithm depends on the specific requirements of the application. For example, in a formula student racecar application, the detection of cones is critical, and a highly reliable and efficient algorithm, such as RangeNet++ or , may be preferred.

The perception pipeline in LiDAR cameras is designed to learn an end-to-end mapping from the point cloud data to the object detections. The objective is to accurately detect, classify, and localize objects in the scene, providing valuable information for the navigation and control of autonomous systems.

<h2>Point cloud processing</h2>
A point cloud is a collection of data points in 3D space that represent the surfaces of objects and scenes. LiDAR data, specifically, provides a high-resolution and dense representation of the environment, making it ideal for a wide range of applications such as autonomous vehicles, mapping, and robotics. However, the raw data from a LiDAR sensor can be overwhelming, with millions of points, making it necessary to use algorithms to extract useful information and insights. 

*Inventaris of known algorithms:* 

1.  RangeNet++
2.  Point-GNN
3.  Dynamic Fused-PointNet (DFPN)
4.  PointGrid
5.  PointContrast
6.  PointSIFT
7.  PointGroup
8.  PointSENet
9.  HybridNet
10.  PointWeb
11.  PointNet++
12.  VoxelNet
13.  PointNet
14.  SECOND
15.  PointRCNN
16.  F-PointNet
17.  PointPillars
18.  Part-A^2 Net
19.  Frustum PointNet
20.  Point-Voxel CNN

All the algorithms listed above are state-of-the-art algorithms for point cloud processing, and are used for various applications in the field of autonomous vehicles, robotics, and computer vision. The choice of an algorithm depends on the specific requirements of the formula student race car and the type of data generated by the LiDAR camera. Some of these algorithms, such as RangeNet++, PointNet, and PointRCNN, are designed for object detection, while others, such as Dynamic Fused-PointNet (DFPN) and PointContrast, are designed for semantic segmentation. PointNet++, PointSENet, and HybridNet are designed to handle large-scale point cloud data. On the other hand, PointPillars and Frustum PointNet are designed specifically for 3D object detection from lidar point clouds. Each algorithm has its own strengths and weaknesses, and the best one for a particular use case should be chosen based on the desired accuracy, processing speed, and computational requirements.

Some of these algorithms have proven to be popular and efficient in point cloud processing: Rangenet++, PointNet++, VoxelNet, and PointRCNN. These algorithms have demonstrated good performance in terms of accuracy and processing speed, while being robust and flexible. 

1.  [RangeNet++](http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/milioto2019iros.pdf): Rangenet++ is an efficient and scalable deep learning-based method for 3D object detection. It uses a hybrid approach that combines range images with voxel representations to achieve high accuracy and real-time performance.
    
2.  [PointNet++](https://github.com/charlesq34/pointnet2): PointNet++ is a hierarchical network that operates on point clouds directly, without the need for pre-processing such as voxelization. It is capable of handling large amounts of data and achieving high accuracy in 3D object detection and semantic segmentation tasks.
    
3.  VoxelNet: VoxelNet is a deep learning-based approach that converts point clouds into a voxel representation and then uses 3D convolutional neural networks for object detection. It is capable of handling large amounts of data and achieving high accuracy, but may be less efficient than other methods.
    
4.  PointRCNN: PointRCNN is a two-stage framework for 3D object detection that first generates region proposals from point clouds and then refines the proposals using 3D convolutional neural networks. It achieves high accuracy and is capable of handling large amounts of data, but may be computationally intensive.


*--- Mogelijkheid om performantie van deze na te gaan op de **KITTI dataset***


